{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORK IN PROGRESS\n",
    "\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Welcome to the Transformer implementation tutorial using PyTorch! In this tutorial, you'll embark on an exciting journey to understand and build one of the most powerful architectures in the field of deep learning.\n",
    "\n",
    "Transformers have revolutionized various natural language processing tasks, such as machine translation, text generation, and sentiment analysis, by capturing long-range dependencies and contextual information effectively. Understanding how to implement a Transformer from scratch will not only deepen your knowledge of deep learning but also equip you with the skills to tackle a wide range of sequence-based tasks.\n",
    "\n",
    "Throughout this tutorial, we'll break down the core components of the Transformer architecture, including self-attention mechanisms, positional encodings, and feed-forward networks. You'll learn how to build each component step-by-step using PyTorch.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"transformerEncoder.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer of transformers is composed of two parts: the multi-head attention and the multi-layer perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"transformerLayer.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Multilayer Perceptron (MLP)](#Multilayer-Perceptron-(MLP))\n",
    "2. [Multi-Head Attention](#Multi-Head-Attention)\n",
    "3. [Transformer Blocks](#Transformer-Blocks)\n",
    "4. [Positional Embedding](#Positional-Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll delve into the role of the Multilayer Perceptron (MLP) within the Transformer architecture. The MLP serves as the core component for processing each position in the sequence independently, enabling the model to capture complex patterns and relationships within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=768, ratio=4.0, dropout=0.0, activation=nn.GELU):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.ratio = ratio\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation()\n",
    "\n",
    "        self.fc1 = nn.Linear(dim, int(dim*ratio))\n",
    "        self.fc2 = nn.Linear(int(dim*ratio), dim)\n",
    "\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
